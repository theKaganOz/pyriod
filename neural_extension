import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from deap import base, creator, tools, algorithms
import random


def create_neural_network(input_dim, layers_config, activation='relu', dropout_rate=0.0, learning_rate=0.001):
    """
    Dynamically creates a neural network model.

    Parameters:
    - input_dim (int): Number of input features
    - layers_config (list of int): Number of neurons in each layer
    - activation (str): Activation function
    - dropout_rate (float): Dropout rate (0.0 means no dropout)
    - learning_rate (float): Learning rate for Adam optimizer

    Returns:
    - model (Sequential): Compiled Keras model
    """
    model = Sequential()
    for i, units in enumerate(layers_config):
        if i == 0:
            model.add(Dense(units, activation=activation, input_dim=input_dim))
        else:
            model.add(Dense(units, activation=activation))
        if dropout_rate > 0:
            model.add(Dropout(dropout_rate))

    model.add(Dense(1))  # Output layer
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')
    return model


def optimize_neural_network(X, y, n_generations=10, population_size=10, input_dim=None):
    """
    Uses NSGA-II to optimize the neural network architecture.

    Objectives:
    - Minimize validation MSE
    - Minimize total number of parameters (as complexity penalty)

    Parameters:
    - X (np.array): Feature set
    - y (np.array): Target
    - n_generations (int): Number of evolutionary steps
    - population_size (int): Size of each generation
    - input_dim (int): Number of features (optional, inferred from X)

    Returns:
    - best_individual (list): Best layer configuration
    - stats (dict): Evolution history
    """
    input_dim = input_dim or X.shape[1]

    def evaluate(individual):
        # Decode architecture
        layers = [int(neurons) for neurons in individual if neurons > 0]
        if len(layers) == 0:
            return 1e6, 1e6  # Penalize empty models

        model = create_neural_network(input_dim, layers)
        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)
        model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)
        pred = model.predict(X_val).flatten()
        mse = mean_squared_error(y_val, pred)
        n_params = model.count_params()
        return mse, n_params

    creator.create("FitnessMulti", base.Fitness, weights=(-1.0, -1.0))  # Minimize both
    creator.create("Individual", list, fitness=creator.FitnessMulti)

    toolbox = base.Toolbox()
    toolbox.register("attr_int", random.randint, 0, 128)
    toolbox.register("individual", tools.initRepeat, creator.Individual, toolbox.attr_int, n=5)
    toolbox.register("population", tools.initRepeat, list, toolbox.individual)
    toolbox.register("evaluate", evaluate)
    toolbox.register("mate", tools.cxTwoPoint)
    toolbox.register("mutate", tools.mutGaussian, mu=64, sigma=32, indpb=0.5)
    toolbox.register("select", tools.selNSGA2)

    pop = toolbox.population(n=population_size)
    stats = tools.Statistics(lambda ind: ind.fitness.values)
    stats.register("avg", np.mean, axis=0)
    stats.register("min", np.min, axis=0)

    algorithms.eaMuPlusLambda(pop, toolbox, mu=population_size, lambda_=population_size, cxpb=0.5, mutpb=0.3,
                               ngen=n_generations, stats=stats, verbose=True)

    best = tools.selBest(pop, 1)[0]
    return best, stats
